## tensorflow基础

使用图 (`graphs`) 来表示计算.
 在会话 (`Session`) 中执行图.
使用张量 (`tensors`) 来代表数据.
通过变量 (`Variables`) 维护状态.
 使用供给 (`feeds`)和取回 (`fetches`) 将数据传入或传出任何操作 

图中的节点为op（operation），一个op获得0或者数个tensors计算，产生0或多个tensors；

tensors张量是按类型划分的多维数组。图在session()中启动，分发op到Devices()中，

在 Python 语言中, 将返回numpy的ndarray 对象; 在 C 和 C++ 语言中, 将返回tensorflow::Tensor实例 


## TensorFlow Object Detection API 


https://github.com/priya-dwivedi/Deep-Learning/blob/master/Object_Detection_Tensorflow_API.ipynb

https://github.com/tensorflow/models/tree/master/research

### 简述 

- 一个可训练性检测模型的集合，包括：
- 带有 **MobileNets** 的 SSD（Single Shot Multibox Detector）
- 带有 Inception V2 的 SSD
- 带有 Resnet 101 的 R-FCN（Region-Based Fully Convolutional Networks）
- 带有 Resnet 101 的 Faster RCNN
- 带有 Inception Resnet v2 的 Faster RCNN
- 上述每一个模型的冻结权重（在 COCO 数据集上训练）可被用于开箱即用推理。
- 一个 Jupyter notebook 可通过我们的模型之一执行开箱即用的推理
- 借助谷歌云实现便捷的本地训练脚本以及分布式训练和评估管道

coco数据集http://mscoco.org/

####安装protoc 
[安装](https://github.com/google/protobuf/releases) ,覆盖到usr/bin，可以先备份/usr/bin/protoc。备份： cp /usr/bin/protoc ~/protoc_bak 
**protocbuf 文件编译**
```text
cd ~/tf_model/model
protoc object_detection/protos/*.proto --python_out=.
```
error:
1.
把tensorflow/model 和slim文件夹添加到**PYTHONPATH**中

```text
export PYTHONPATH=$PYTHONPATH:`pwd`:`pwd`/slim
```

 python object_detection/builders/model_builder_test.py

2.
conv_defs = copy.copy(mobilenet_v1.MOBILENETV1_CONV_DEFS)
AttributeError: module 'nets.mobilenet_v1' has no attribute 'MOBILENETV1_CONV_DEFS'

打开research/object_detection/models/ssd_mobilenet_v1_fpn_feature_extractor.py，把34行copy.copy函数的第二个参数改一下。

 `conv_defs = copy.copy(mobilenet_v1._CONV_DEFS)#mobilenet_v1.MOBILENETV1_CONV_DEFS)`

注意，这里site-packages里也有

3.

注意不同python库路径冲突问题

还可以在文件开头改：

```
import sys
sys.path.append('/home/js/workspace/python/models/research')
sys.path.append('/home/js/workspace/python/models/research/object_detection')
sys.path.append('/home/js/workspace/python/models/research/object_detection/protos')
sys.path.remove('/home/js/new_lidar/devel/lib/python2.7/dist-packages')
sys.path.remove('/opt/ros/kinetic/lib/python2.7/dist-packages')
```



### Nvidia driver

最简单的方法直接apt安装，用四个命令：

```
sudo apt-get remove --purge nvidia*
sudo add-apt-repository ppa:graphics-drivers/ppa
sudo apt-get update
sudo apt-get install nvidia-384（自己的版本）
```

https://blog.csdn.net/fdqw_sph/article/details/78745375    tick

https://blog.csdn.net/changer_sun/article/details/79219419

https://blog.csdn.net/victoryaoyu/article/details/70034569 1050ti

https://developer.nvidia.com/cuda-90-download-archive

  https://blog.csdn.net/linhai1028/article/details/79233311 tensorflow gpu

https://blog.csdn.net/zz2230633069/article/details/80782692 keras gpu

```
import os
os.environ["CUDA_VISIBLE_DEVICES"] = "1"
```

监控

`watch -n 2 nvidia-smi`

### 

### instance

https://blog.csdn.net/asukasmallriver/article/details/78696260

https://blog.csdn.net/asukasmallriver/article/details/78752178#8%E6%91%84%E5%83%8F%E5%A4%B4%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B

https://zhuanlan.zhihu.com/p/30347173 识别视频

https://zhuanlan.zhihu.com/p/27469690 数据集

https://zhuanlan.zhihu.com/p/29215867 较全的实践





原文：https://blog.csdn.net/asukasmallriver/article/details/78752178 

create_pascal_tf_record.py第160行 :

```
examples_path = os.path.join(data_dir, year, 'ImageSets', 'Main', 'aeroplane_' + FLAGS.set + '.txt')
```

改为：

```
examples_path = os.path.join(data_dir, year, 'ImageSets', 'Main/' + FLAGS.set + '.txt')
```



---------------------


```bash
    python dataset/create_pascal_tf_record.py \
        --data_dir=dataset/VOCtrainval_06-Nov-2007/VOCdevkit \
        --year=VOC2007 \
        --set=train \
        --output_path=record/pascal_train.record

    python dataset/create_pascal_tf_record.py \
        --data_dir=dataset/VOCtrainval_06-Nov-2007/VOCdevkit \
        --year=VOC2007 \
        --set=val \
        --output_path=record/pascal_val.record
```



这个create_pascal_tf_record.py做的事情分为三个部分

- 将每张图片注释参数（图片的宽度与高度，对象边界框，类名称，…等）跟标签映射（类ID跟类名称的对应关系）读出来并塞进tf.train.Example协议缓冲区
- 将tf.train.Example协议缓冲区序列化为字符串
- 最后tf.python_io.TFRecordWriter把字符 串写入TFRecords

直接从项目中复制一个样本出来改（object_detection/samples/configs/）

修改`ssd_inception_v2_coco.config`的关键语句：


label文件 官方已经有提供放在 object_detection/pascal_val.record

```text
train_input_reader: {
tf_record_input_reader { input_path: "PATH_TO_BE_CONFIGURED/pascal_train.record" }
label_map_path: "PATH_TO_BE_CONFIGURED/pascal_label_map.pbtxt"}
```

指定测试数据的label和record数据文件



```text
eval_input_reader: {
tf_record_input_reader { input_path: "PATH_TO_BE_CONFIGURED/pascal_val.record" }
label_map_path: "PATH_TO_BE_CONFIGURED/pascal_label_map.pbtxt"
}
```







如果GPU内存不够大，务必使用CPU clones
```
flags.DEFINE_boolean('clone_on_cpu', True,
                    'Force clones to be deployed on CPU.  Note that even if '
                   'set to False (allowing ops to run on gpu), some ops may '
                     'still be run on the CPU if they have no GPU kernel.')
```



- 训练模型输出文件夹：

```
flags.DEFINE_string('train_dir', 'train',
                    'Directory to save the checkpoints and training summari
```

设置pipeline_config_path：
```
flags.DEFINE_string('pipeline_config_path', 'ssd_inception_v2_coco.config',
                   'Path to a pipeline_pb2.TrainEvalPipelineConfig config '
                    'file. If provided, other configs are ignored')
```

根目录

```
python train.py --logtostderr 
```

```
tensorboard --logdir=train
```

或者这种形式

```text
python object_detection/train.py \
   --logtostderr \
   --pipeline_config_path=${定义的Config} \
   --train_dir=${训练结果要存放的目录}
```



可选：测试

```text
python object_detection/train.py \
   --logtostderr \
   --pipeline_config_path=${定义的Config} \
   --train_dir=${训练结果要存放的目录}
```

生成pb文件

    将train文件夹下的如下文件复制到pb文件夹下，并去除ckpt后面的“-数字”，checkpoint文件内相应也要改：
```
checkpoint
model.ckpt.data-00000-of-00001
model.ckpt.index
model.ckpt.meta
```

    在项目根目录下执行：
```
python export_inference_graph.py \
--pipeline_config_path ssd_inception_v2_coco.config \
--trained_checkpoint_prefix pb/model.ckpt \
--output_directory pb
```


    在pb目录下可以找到生成的pb文件：

frozen_inference_graph.pb

8.摄像头目标检测

    修改webcamdetect.py文件：

PATH_TO_CKPT = 'pb/frozen_inference_graph.pb'  





**得到映射关系的文件** 
可以在**object_detection/data**路径下看到为我们提供了一些文件，这些文件后缀为**.pbtxt**，里面有VOC数据的映射文件，所以就直接拿过来用啦

**fine_tune_checkpoint: “PATH_TO_BE_CONFIGURED/model.ckpt”** 
这一个地方可以提供预训练的权重，有这样的模型文件的话可以让我们的训练过程更快，官方也提供了一些预训练好的模型

**pipeline_config_path** 为我们使用的model配置文件 
**train_dir**自己设定的在训练过程中记录模型的文件夹 

**eval_dir**就是自己设定的存放评估时做记录的路径 
**checkpoint_dir**就是训练的时候的模型记录的路径即**train_dir**

http://127.0.0.1:6006/



#### 总结

改配置，改路径

路径可以改程序，也可以改命令



### voc数据集 

原文：https://blog.csdn.net/gulingfengze/article/details/79639111 

* Annotations文件夹
  该文件下存放的是xml格式的标签文件，每个xml文件都对应

  JPEGImages文件夹的一张图片。

* JPEGImages文件夹
    改文件夹下存放的是数据集图片，包括训练和测试图片。
* ImageSets文件夹
    该文件夹下存放了三个文件，分别是Layout、Main、Segmentation。在这里我们只用存放图像数据的Main文件，其他两个暂且不管。
* SegmentationClass文件和SegmentationObject文件。
    这两个文件都是与图像分割相关。

只需要：Annotations文件夹、JPEGImages文件夹、ImageSets文件夹下的Main文件

1. 创建一个VOCdevkit文件夹，下面再创建Annotations、JPEGImages、ImageSets三个文件夹，最后在ImageSets文件夹下再创建一个Main文件夹。 

   数据集图片都放到JPEGImages文件夹下。按照习惯，我们将图片的名字修改为000001.jpg这种格式的（参照原始数据集图片命名规则）格式jpg，大小适中，参考原图

2. 制作Annotations文件夹下所需要存放的xml文件

   [LabelImg工具](https://github.com/tzutalin/labelImg)

`lxml` 库文件还是要装的

每标注完一张图片后进行保存，保存的xml文件名要与对应图片名一致，大家可以参考原始VOC2007数据集中JPEGImages文件夹下图片的命名和Annotations文件夹中的xml文件命名规则。
备注：这里还有个制作工具VOC2007[https://pan.baidu.com/s/1EBbX9Phy8BTRrWrmEfQnsw]数据格式制作工具 也很好用，大家也可以试一试。这个是在网上看到的，忘记作者了，在这里表示感谢。 







第三步：我们来制作ImageSets文件夹下Main文件夹中的4个文件（test.txt、train.txt、trainval.txt、val.txt）。
首先我们先来了解下这四个文件到底是干什么用的，当然从文件的命名上我们也都能大体猜得上来他们的作用，不过这里还是简单的说明一下吧。
test.txt：测试集
train.txt：训练集
val.txt：验证集
trainval.txt：训练和验证集

在原始VOC2007数据集中，trainval大约占整个数据集的50%，test大约为整个数据集的50%；train大约是trainval的50%，val大约为trainval的50%。所以我们可参考以下代码来生成这4个txt文件：



```python
import os  
import random

trainval_percent = 0.5 
train_percent = 0.5 
xmlfilepath = 'Annotations' 
txtsavepath = 'ImageSets/Main' 
total_xml = os.listdir(xmlfilepath) 

num=len(total_xml) 
list=range(num) 
tv=int(num*trainval_percent)
tr=int(tv*train_percent) 
trainval= random.sample(list,tv) 
train=random.sample(trainval,tr) 

ftrainval = open(txtsavepath+'/trainval.txt', 'w') 
ftest = open(txtsavepath+'/test.txt', 'w') 
ftrain = open(txtsavepath+'/train.txt', 'w') 
fval = open(txtsavepath+'/val.txt', 'w') 


for i in list: 
    name=total_xml[i][:-4]+'\n' 
    if i in trainval: 
        ftrainval.write(name) 
        if i in train: 
            ftrain.write(name)
        else: 
             fval.write(name) 
    else: 
        ftest.write(name) 
        
ftrainval.close() 
ftrain.close() 
fval.close() 
ftest .close()

```















## Cuda



https://developer.nvidia.com/cuda-80-download-archive nvidia官方下载

https://www.jianshu.com/p/c5ee19cdc9b6/

https://www.cnblogs.com/xia-Autumn/p/6228911.html

```
export LD_LIBRARY_PATH="$LD_LIBRARY_PATH:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64"
export CUDA_HOME=/usr/local/cuda
```



### 测试

nvcc --version

cd  /usr/local/cuda-9.0/samples/1_Utilities/deviceQuery

sudo make

./deviceQuery

```
export CUDA_VISIBLE_DEVICES="0"
```



InvalidArgumentError (see above for traceback): NodeDef mentions attr 'data_format' not in Op<name=DepthwiseConv2dNative;

大致可以猜测到新的mobilenet由tensorflow 1.6构造，已经不兼容tensorflow 1.4了。 



ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory

1.11只支持cuda9





### cv2： 

`v4l2-ctl -d  /dev/video0 --all `有问题

改用：

```python
#cap.set(cv2.cv.CV_CAP_PROP_FRAME_WIDTH,1280)
#cap.set(cv2.cv.CV_CAP_PROP_FRAME_HEIGHT,480)
cap.set(3,1920) #
cap.set(4,1080)
```





https://blog.csdn.net/yjfncu/article/details/81512697?utm_source=blogxgwz0 ssd_mobile tfrecord

https://pan.baidu.com/s/1WO2OmMFyyBvqeOxtX9-xyQ create tf

https://github.com/tensorflow/models/pull/5354 name

https://github.com/tensorflow/models/pull/5354/commits/7f8b2ff37fc14bdd08800395c86a442822900da2





## svm



https://blog.csdn.net/u012874151/article/details/45457085
